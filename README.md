# Speech Resource List

This reading list is mantained by [Ramon Sanabria](http://www.cs.cmu.edu/~ramons/), [Edinburgh NLP](https://edinburghnlp.inf.ed.ac.uk/) and [The Centre for Speech Technology Research](http://www.cstr.ed.ac.uk/), [The University of Edinburgh](https://www.ed.ac.uk/). Ex [Language Technologies Institute](https://www.lti.cs.cmu.edu/) and [Robotics Institute](https://www.ri.cmu.edu/), [CMU](https://www.cmu.edu/). 

This list is probably biased towards my current research directions. So, please, if there is anythin missing, please let me know. Suggestions are super welcome :)



<h1 id="papers">Papers</h1>

* [Multimodal](#multimodal)
	* [Adaptation](#multimodal_adaptation)
	* [Representation](#multimodal_adaptation)
	* [Unsupervised](#multimodal_unsupervised)
* [Unsupervised](#unsupervised)
	* [Segmentation](#unsupervised_segmentation)

<h2 id="multimodal">Multimodal</h2>
<h3 id="multimodal_adaptation">Adaptation</h2>

* Moriya Y, Jones GJ. [Multimodal Speaker Adaptation of Acoustic Model and Language Model for Asr Using Speaker Face Embedding.](https://ieeexplore.ieee.org/abstract/document/8683724) **ICASSP 2019** 
* Caglayan O, Sanabria R, Palaskar S, Barraul L, Metze F. [Multimodal Grounding for Sequence-to-sequence Speech Recognition.](https://arxiv.org/pdf/1811.03865.pdf) **ICASSP 2019**
* Palaskar S, Sanabria R, Metze F. [End-to-end multimodal speech recognition.](https://arxiv.org/abs/1804.09713) **ICASSP 2018**
* Gupta A, Miao Y, Neves L, Metze F. [Visual features for context-aware speech recognition.](https://arxiv.org/abs/1712.00489) **ICASSP 2017** 
* Sun F, Harwath D, Glass J. [Look, Listen, And Decode: Multimodal Speech Recognition With Images.](https://groups.csail.mit.edu/sls/publications/2016/FelixSun_SLT_2016.pdf) **ICASSP 2016**

<h3 id="multimodal_representation">Representation</h2>

<h3 id="multimodal_unsupervised">Unsupervised</h2>

* Kamper H, Settle S, Shakhnarovich G, Livescu K. [Visually grounded learning of keyword prediction from untranscribed speech.](https://arxiv.org/pdf/1703.08136.pdf) **INTERSPEECH 2017**

<h2 id="unsupervised">Unsupervised</h2>
<h3 id="unsupervised_segmentation">Segmentation</h2>

* Kamper H, Jansen A, Goldwater S. [Fully unsupervised small-vocabulary speech recognition using a segmental bayesian model.](https://pdfs.semanticscholar.org/26f5/b704a3f1e0d8d7187c048ca48630553684b6.pdf) **Interspeech 2015**


<h2 id="unsupervised_representation">Representation</h2>
* Kamper H, Jansen A, King S, Goldwater S. [Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.724.7106&rep=rep1&type=pdf) **SLT 2014**

# Tutorials

# Courses
* [The University of Edinburgh, AUTOMATIC SPEECH RECOGNITION](http://www.inf.ed.ac.uk/teaching/courses/asr/lectures-2019.html)

# Journals

# Workshops


# Datasets
* Great resource from [@_josh_meyer_](https://twitter.com/_josh_meyer_) [here](https://github.com/JRMeyer/open-speech-corpora)
* Sanabria R, Caglayan O, Palaskar S, Elliott D, Barrault L, Specia L, Metze F. [How2: a large-scale dataset for multimodal language understanding.](https://arxiv.org/pdf/1811.00347.pdf). *NIPS 2018 Workshop*. 


